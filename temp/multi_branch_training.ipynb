{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    \n",
    "from utils import create_sliding_windows, SequentialMIONetDataset\n",
    "from s_mionet import SequentialMIONet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load neutron monitoring data\n",
    "input_data = np.load('data/neutron_data_22yrs.npy')\n",
    "trunk = np.load('data/grid_points.npy')\n",
    "target = np.load('data/dose_array.npy')\n",
    "\n",
    "# Normalize trunk input\n",
    "trunk[:, 0] = (trunk[:, 0] - np.min(trunk[:, 0])) / (np.max(trunk[:, 0]) - np.min(trunk[:, 0]))\n",
    "trunk[:, 1] = (trunk[:, 1] - np.min(trunk[:, 1])) / (np.max(trunk[:, 1]) - np.min(trunk[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input shape: (4017, 12)\n",
      "Validation input shape: (4018, 12)\n",
      "Test input shape: (365, 12)\n"
     ]
    }
   ],
   "source": [
    "def train_val_test_split(input_data, target):\n",
    "    # Define the number of test samples (last 365 days)\n",
    "    test_size = 365\n",
    "\n",
    "    # Split data into training+validation and test\n",
    "    train_val_input = input_data[:-test_size]\n",
    "    train_val_target = target[:-test_size]\n",
    "    test_input = input_data[-test_size:]\n",
    "    test_target = target[-test_size:]\n",
    "\n",
    "    # Calculate split index for training and validation\n",
    "    train_size = int(len(train_val_input) * 0.5)  # 80% for training\n",
    "    val_size = len(train_val_input) - train_size  # 20% for validation\n",
    "\n",
    "    # Training set\n",
    "    train_input = train_val_input[:train_size]\n",
    "    train_target = train_val_target[:train_size]\n",
    "\n",
    "    # Validation set\n",
    "    val_input = train_val_input[train_size:]\n",
    "    val_target = train_val_target[train_size:]\n",
    "\n",
    "    # Final shapes check\n",
    "    print(\"Train input shape:\", train_input.shape)\n",
    "    print(\"Validation input shape:\", val_input.shape)\n",
    "    print(\"Test input shape:\", test_input.shape)\n",
    "\n",
    "    return train_input, train_target, val_input, val_target, test_input, test_target\n",
    "\n",
    "# Assuming input_data and target are defined elsewhere in the notebook\n",
    "train_input, train_target, val_input, val_target, test_input, test_target = train_val_test_split(input_data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data normalization (min-max scaling)\n",
    "scaler = MinMaxScaler()\n",
    "train_input = scaler.fit_transform(train_input)\n",
    "val_input = scaler.transform(val_input)\n",
    "test_input = scaler.transform(test_input)\n",
    "\n",
    "# target data normalization (min-max scaling)\n",
    "scaler_target = MinMaxScaler()\n",
    "train_target = scaler_target.fit_transform(train_target)[..., np.newaxis]\n",
    "val_target = scaler_target.transform(val_target)[..., np.newaxis]\n",
    "test_target = scaler_target.transform(test_target)[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window size: 90\n",
      "Check the shapes of the generated sequences\n",
      "-----------------------------------------\n",
      "Train input shape: torch.Size([3928, 90, 12])\n",
      "Train target shape: torch.Size([3928, 65341, 1])\n",
      "Validation input shape: torch.Size([3929, 90, 12])\n",
      "Validation target shape: torch.Size([3929, 65341, 1])\n",
      "Test input shape: torch.Size([276, 90, 12])\n",
      "Test target shape: torch.Size([276, 65341, 1])\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "window_size = 90\n",
    "print(\"Window size:\", window_size)\n",
    "\n",
    "# Generate sequences for the training set\n",
    "train_input_seq, train_target_seq = create_sliding_windows(train_input, train_target, window_size)\n",
    "\n",
    "# Generate sequences for the testing set\n",
    "test_input_seq, test_target_seq = create_sliding_windows(test_input, test_target, window_size)\n",
    "\n",
    "# generate sequences for the validation set\n",
    "val_input_seq, val_target_seq = create_sliding_windows(val_input, val_target, window_size)\n",
    "\n",
    "\n",
    "# print the shapes of the generated sequences\n",
    "print(\"Check the shapes of the generated sequences\\n-----------------------------------------\")\n",
    "print(\"Train input shape:\", train_input_seq.shape)\n",
    "print(\"Train target shape:\", train_target_seq.shape)\n",
    "print(\"Validation input shape:\", val_input_seq.shape)\n",
    "print(\"Validation target shape:\", val_target_seq.shape)\n",
    "print(\"Test input shape:\", test_input_seq.shape)\n",
    "print(\"Test target shape:\", test_target_seq.shape)\n",
    "print(\"-----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create DataLoaders for training and validation sets\n",
      "-----------------------------------------\n",
      "Batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders for training and validation sets\n",
    "print(\"Create DataLoaders for training and validation sets\\n-----------------------------------------\")\n",
    "batch_size = 16\n",
    "print(\"Batch size:\", batch_size)\n",
    "\n",
    "train_dataset = SequentialMIONetDataset(train_input_seq, trunk, train_target_seq)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "val_dataset = SequentialMIONetDataset(val_input_seq, trunk, val_target_seq)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = SequentialMIONetDataset(test_input_seq, trunk, test_target_seq)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    dim = 128\n",
    "\n",
    "    # Define a single branch configuration to be reused\n",
    "    base_branch_config = {\n",
    "        \"type\": \"lstm\",\n",
    "        \"input_size\": 1,\n",
    "        \"hidden_size\": 128,\n",
    "        \"num_layers\": 4,\n",
    "        \"output_size\": dim\n",
    "    }\n",
    "\n",
    "    # Create a dictionary with the same branch configuration for 12 branches\n",
    "    branches_config = {f\"sensor{i+1}\": base_branch_config for i in range(12)}\n",
    "\n",
    "    # Trunk network configuration\n",
    "    trunk_architecture = [2, 128, 128, dim]\n",
    "    num_outputs = 1\n",
    "\n",
    "    # Instantiate the model with the replicated branches\n",
    "    model = SequentialMIONet(branches_config, trunk_architecture, num_outputs,use_transform=False)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequentialMIONet(\n",
      "  (branches): ModuleDict(\n",
      "    (sensor1): LSTM(\n",
      "      (lstm): LSTM(1, 128, num_layers=4, batch_first=True)\n",
      "      (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (sensor2): LSTM(\n",
      "      (lstm): LSTM(1, 128, num_layers=4, batch_first=True)\n",
      "      (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (sensor3): LSTM(\n",
      "      (lstm): LSTM(1, 128, num_layers=4, batch_first=True)\n",
      "      (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (sensor4): LSTM(\n",
      "      (lstm): LSTM(1, 128, num_layers=4, batch_first=True)\n",
      "      (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (sensor5): LSTM(\n",
      "      (lstm): LSTM(1, 128, num_layers=4, batch_first=True)\n",
      "      (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (sensor6): LSTM(\n",
      "      (lstm): LSTM(1, 128, num_layers=4, batch_first=True)\n",
      "      (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (sensor7): LSTM(\n",
      "      (lstm): LSTM(1, 128, num_layers=4, batch_first=True)\n",
      "      (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (sensor8): LSTM(\n",
      "      (lstm): LSTM(1, 128, num_layers=4, batch_first=True)\n",
      "      (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (sensor9): LSTM(\n",
      "      (lstm): LSTM(1, 128, num_layers=4, batch_first=True)\n",
      "      (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (sensor10): LSTM(\n",
      "      (lstm): LSTM(1, 128, num_layers=4, batch_first=True)\n",
      "      (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (sensor11): LSTM(\n",
      "      (lstm): LSTM(1, 128, num_layers=4, batch_first=True)\n",
      "      (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (sensor12): LSTM(\n",
      "      (lstm): LSTM(1, 128, num_layers=4, batch_first=True)\n",
      "      (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (trunk_net): FCN(\n",
      "    (network): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = init_model().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'multi_branch/lstm_window_{window_size}.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'multi_branch/array'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, scaler, device='cuda'):\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for branch_batch, trunk_batch, target_batch in test_loader:\n",
    "            branch_batch, trunk_batch, target_batch = (\n",
    "                {key: value.to(device) for key, value in branch_batch.items()},\n",
    "                trunk_batch.to(device),\n",
    "                target_batch.to(device),\n",
    "            )\n",
    "            output = model(branch_batch, trunk_batch)\n",
    "            all_preds.append(output.cpu().numpy())\n",
    "            all_targets.append(target_batch.cpu().numpy())\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "    \n",
    "    print(\"All predictions shape before reshape:\", all_preds.shape)\n",
    "    print(\"All targets shape before reshape:\", all_targets.shape)\n",
    "\n",
    "    # Reshape to 2D (n_samples, n_features) for inverse scaling\n",
    "    all_preds = all_preds.reshape(all_preds.shape[0], -1)\n",
    "    all_targets = all_targets.reshape(all_targets.shape[0], -1)\n",
    "\n",
    "    print(\"All predictions shape after reshape:\", all_preds.shape)\n",
    "    print(\"All targets shape after reshape:\", all_targets.shape)\n",
    "\n",
    "    # Inverse scaling\n",
    "    all_preds = scaler.inverse_transform(all_preds)\n",
    "    all_targets = scaler.inverse_transform(all_targets)\n",
    "    \n",
    "    # Compute metrics for each sample\n",
    "    rmse, mae, r2, l2_error = [], [], [], []\n",
    "    for i in range(all_preds.shape[0]):\n",
    "        rmse.append(np.sqrt(np.mean((all_preds[i] - all_targets[i]) ** 2)))\n",
    "        mae.append(np.mean(np.abs(all_preds[i] - all_targets[i])))\n",
    "        r2.append(1 - np.sum((all_preds[i] - all_targets[i]) ** 2) / np.sum((all_targets[i] - np.mean(all_targets[i])) ** 2))\n",
    "        l2_error.append(np.linalg.norm(all_preds[i] - all_targets[i], 2) / np.linalg.norm(all_targets[i], 2) * 100)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    rmse = np.array(rmse)\n",
    "    mae = np.array(mae)\n",
    "    r2 = np.array(r2)\n",
    "    l2_error = np.array(l2_error)\n",
    "    \n",
    "    # Save the results to a file\n",
    "    results = np.stack((rmse, mae, r2, l2_error), axis=1)\n",
    "    #save_path = os.path.join(save_dir, f'lstm_window_{window_size}_results.npy')\n",
    "    #np.save(save_path, results)\n",
    "    #print(f\"Results saved to {save_path}\")\n",
    "    \n",
    "    # Compute average metrics\n",
    "    rmse = np.mean(rmse)\n",
    "    mae = np.mean(mae)\n",
    "    r2 = np.mean(r2)\n",
    "    l2_error = np.mean(l2_error)\n",
    "\n",
    "    print(f\"Final Model Evaluation on Test Set:\")\n",
    "    print(f\"RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}, L2 Error: {l2_error:.4f}\")\n",
    "\n",
    "    return rmse, mae, r2, l2_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All predictions shape before reshape: (276, 65341, 1)\n",
      "All targets shape before reshape: (276, 65341, 1)\n",
      "All predictions shape after reshape: (276, 65341)\n",
      "All targets shape after reshape: (276, 65341)\n",
      "Final Model Evaluation on Test Set:\n",
      "RMSE: 0.0003, MAE: 0.0003, R²: 0.9816, L2 Error: 0.8948\n",
      "(0.00029908805, 0.0002630907, 0.9815862175733934, 0.8947618552011208)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(model, test_loader, scaler_target, device=device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch-env]",
   "language": "python",
   "name": "conda-env-.conda-pytorch-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
