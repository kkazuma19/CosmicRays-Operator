{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    \n",
    "from utils import create_sliding_windows, SequentialDeepONetDataset\n",
    "from s_deeponet import SequentialDeepONet\n",
    "from deeponet import DeepONet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load neutron monitoring data\n",
    "input_data = np.load('data/neutron_data_22yrs.npy')\n",
    "trunk = np.load('data/grid_points.npy')\n",
    "target = np.load('data/dose_array.npy')\n",
    "\n",
    "# Normalize trunk input\n",
    "trunk[:, 0] = (trunk[:, 0] - np.min(trunk[:, 0])) / (np.max(trunk[:, 0]) - np.min(trunk[:, 0]))\n",
    "trunk[:, 1] = (trunk[:, 1] - np.min(trunk[:, 1])) / (np.max(trunk[:, 1]) - np.min(trunk[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input shape: (4017, 12)\n",
      "Validation input shape: (4018, 12)\n",
      "Test input shape: (365, 12)\n"
     ]
    }
   ],
   "source": [
    "def train_val_test_split(input_data, target):\n",
    "    # Define the number of test samples (last 365 days)\n",
    "    test_size = 365\n",
    "\n",
    "    # Split data into training+validation and test\n",
    "    train_val_input = input_data[:-test_size]\n",
    "    train_val_target = target[:-test_size]\n",
    "    test_input = input_data[-test_size:]\n",
    "    test_target = target[-test_size:]\n",
    "\n",
    "    # Calculate split index for training and validation\n",
    "    train_size = int(len(train_val_input) * 0.5)  # 80% for training\n",
    "    val_size = len(train_val_input) - train_size  # 20% for validation\n",
    "\n",
    "    # Training set\n",
    "    train_input = train_val_input[:train_size]\n",
    "    train_target = train_val_target[:train_size]\n",
    "\n",
    "    # Validation set\n",
    "    val_input = train_val_input[train_size:]\n",
    "    val_target = train_val_target[train_size:]\n",
    "\n",
    "    # Final shapes check\n",
    "    print(\"Train input shape:\", train_input.shape)\n",
    "    print(\"Validation input shape:\", val_input.shape)\n",
    "    print(\"Test input shape:\", test_input.shape)\n",
    "\n",
    "    return train_input, train_target, val_input, val_target, test_input, test_target\n",
    "\n",
    "# Assuming input_data and target are defined elsewhere in the notebook\n",
    "train_input, train_target, val_input, val_target, test_input, test_target = train_val_test_split(input_data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data normalization (min-max scaling)\n",
    "scaler = MinMaxScaler()\n",
    "train_input = scaler.fit_transform(train_input)\n",
    "val_input = scaler.transform(val_input)\n",
    "test_input = scaler.transform(test_input)\n",
    "\n",
    "# target data normalization (min-max scaling)\n",
    "scaler_target = MinMaxScaler()\n",
    "train_target = scaler_target.fit_transform(train_target)[..., np.newaxis]\n",
    "val_target = scaler_target.transform(val_target)[..., np.newaxis]\n",
    "test_target = scaler_target.transform(test_target)[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window size: 30\n",
      "Check the shapes of the generated sequences\n",
      "-----------------------------------------\n",
      "Train input shape: torch.Size([3988, 30, 12])\n",
      "Train target shape: torch.Size([3988, 65341, 1])\n",
      "Validation input shape: torch.Size([3989, 30, 12])\n",
      "Validation target shape: torch.Size([3989, 65341, 1])\n",
      "Test input shape: torch.Size([336, 30, 12])\n",
      "Test target shape: torch.Size([336, 65341, 1])\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "window_size = 30\n",
    "print(\"Window size:\", window_size)\n",
    "\n",
    "# Generate sequences for the training set\n",
    "train_input_seq, train_target_seq = create_sliding_windows(train_input, train_target, window_size)\n",
    "\n",
    "# Generate sequences for the testing set\n",
    "test_input_seq, test_target_seq = create_sliding_windows(test_input, test_target, window_size)\n",
    "\n",
    "# generate sequences for the validation set\n",
    "val_input_seq, val_target_seq = create_sliding_windows(val_input, val_target, window_size)\n",
    "\n",
    "\n",
    "# print the shapes of the generated sequences\n",
    "print(\"Check the shapes of the generated sequences\\n-----------------------------------------\")\n",
    "print(\"Train input shape:\", train_input_seq.shape)\n",
    "print(\"Train target shape:\", train_target_seq.shape)\n",
    "print(\"Validation input shape:\", val_input_seq.shape)\n",
    "print(\"Validation target shape:\", val_target_seq.shape)\n",
    "print(\"Test input shape:\", test_input_seq.shape)\n",
    "print(\"Test target shape:\", test_target_seq.shape)\n",
    "print(\"-----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create DataLoaders for training and validation sets\n",
      "-----------------------------------------\n",
      "Batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders for training and validation sets\n",
    "print(\"Create DataLoaders for training and validation sets\\n-----------------------------------------\")\n",
    "batch_size = 16\n",
    "print(\"Batch size:\", batch_size)\n",
    "\n",
    "train_dataset = SequentialDeepONetDataset(train_input_seq, trunk, train_target_seq)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "val_dataset = SequentialDeepONetDataset(val_input_seq, trunk, val_target_seq)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = SequentialDeepONetDataset(test_input_seq, trunk, test_target_seq)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from fourier_embedding import FourierFeatures\n",
    "from fcn import FCN\n",
    "from lstm import LSTM, AttentionLSTM\n",
    "from gru import GRU, AttentionGRU\n",
    "from rnn import RNN\n",
    "from transformer import Transformer, Informer\n",
    "\n",
    "class SequentialDeepONet(nn.Module):\n",
    "    def __init__(self, branch_type, branch_input_size, branch_hidden_size, branch_num_layers, branch_output_size, \n",
    "                 trunk_architecture, num_outputs, use_transform=True, activation_fn=nn.ReLU, num_heads=4):\n",
    "        super(SequentialDeepONet, self).__init__()\n",
    "\n",
    "        self.use_transform = use_transform\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        # Initialize the branch network based on the specified type\n",
    "        if branch_type == 'rnn':\n",
    "            self.branch_net = RNN(branch_input_size, branch_hidden_size, branch_num_layers, branch_output_size)\n",
    "        elif branch_type == 'lstm':\n",
    "            self.branch_net = LSTM(branch_input_size, branch_hidden_size, branch_num_layers, branch_output_size)\n",
    "        elif branch_type == 'gru':\n",
    "            self.branch_net = GRU(branch_input_size, branch_hidden_size, branch_num_layers, branch_output_size)\n",
    "        elif branch_type == 'attention_lstm':\n",
    "            self.branch_net = AttentionLSTM(branch_input_size, branch_hidden_size, branch_num_layers, branch_output_size, num_heads=num_heads)\n",
    "        elif branch_type == 'attention_gru':\n",
    "            self.branch_net = AttentionGRU(branch_input_size, branch_hidden_size, branch_num_layers, branch_output_size, num_heads=num_heads)\n",
    "        elif branch_type == 'transformer':\n",
    "            self.branch_net = Transformer(branch_input_size, branch_hidden_size, num_heads, branch_num_layers, branch_output_size)\n",
    "        elif branch_type == 'informer':\n",
    "            self.branch_net = Informer(branch_input_size, branch_hidden_size, num_heads, branch_num_layers, branch_output_size)\n",
    "        elif branch_type == 'fcn':  # New FCN-based branch option\n",
    "            self.branch_net = FCN([branch_input_size] + [branch_hidden_size] * (branch_num_layers - 1) + [branch_output_size], activation_fn)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported branch type: {branch_type}\")\n",
    "\n",
    "        # Trunk network (fully connected using the FCN class)\n",
    "        self.trunk_net = FCN(trunk_architecture, activation_fn)\n",
    "\n",
    "        if self.use_transform:\n",
    "            self.final_layer = nn.Linear(branch_output_size, num_outputs)\n",
    "        else:\n",
    "            self.b = nn.Parameter(torch.zeros(1))  # Optional bias\n",
    "\n",
    "    def forward(self, branch_input, trunk_input):\n",
    "        # Process branch input (sequential data) through the selected branch network\n",
    "        branch_output = self.branch_net(branch_input) \n",
    "        \n",
    "        #print(\"Branch output shape before processing:\", branch_output.shape)\n",
    "\n",
    "        if len(branch_output.shape) == 3:  # If sequential (RNN, LSTM, GRU, etc.)\n",
    "            branch_output = branch_output[:, -1, :]  # Take last time step\n",
    "\n",
    "        #print(\"Branch output shape:\", branch_output.shape)\n",
    "        # Process trunk input (spatial data) through the trunk network\n",
    "        trunk_output = self.trunk_net(trunk_input)\n",
    "        #print(\"Trunk output shape:\", trunk_output.shape)\n",
    "\n",
    "        # Combine branch and trunk outputs using einsum\n",
    "        combined_output = torch.einsum('bi,bpi->bpi', branch_output, trunk_output)  # Shape: (batch_size, num_trunk_points, hidden_size)\n",
    "        print(\"Combined output shape:\", combined_output.shape)\n",
    "        \n",
    "        if self.use_transform:\n",
    "            combined_output = self.final_layer(combined_output)  # Final prediction\n",
    "        else:\n",
    "            combined_output = combined_output.sum(dim=-1, keepdim=True)  # Reduce last dimension\n",
    "            combined_output += self.b  # Add bias\n",
    "        print(\"Final output shape:\", combined_output.shape)\n",
    "\n",
    "        return combined_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    dim = 128\n",
    "    model = SequentialDeepONet(\n",
    "        branch_type='lstm',\n",
    "        branch_input_size=12,\n",
    "        branch_hidden_size=128,\n",
    "        branch_num_layers=4,\n",
    "        branch_output_size=dim,\n",
    "        trunk_architecture=[2, 128, 128, dim],\n",
    "        num_outputs=1,\n",
    "        use_transform=False,\n",
    "        activation_fn=nn.ReLU,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequentialDeepONet(\n",
      "  (branch_net): LSTM(\n",
      "    (lstm): LSTM(12, 128, num_layers=4, batch_first=True)\n",
      "    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (trunk_net): FCN(\n",
      "    (network): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = init_model().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set the hyperparameters\n",
      "-----------------------------------------\n",
      "single_branch/test_fcn_window_30.pth\n",
      "Number of epochs: 100\n",
      "Learning rate: 0.001\n",
      "Patience: 5\n",
      "Loss function: MSELoss()\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Set the hyperparameters\\n-----------------------------------------\")\n",
    "# save path\n",
    "save_dir = 'single_branch/'\n",
    "save_path = os.path.join(save_dir, f'test_fcn_window_{window_size}.pth')\n",
    "print(save_path)\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-3\n",
    "patience = 5\n",
    "\n",
    "print(\"Number of epochs:\", num_epochs)\n",
    "print(\"Learning rate:\", learning_rate)\n",
    "print(\"Patience:\", patience)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "print(\"Loss function:\", criterion)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5, verbose=True)\n",
    "#print(\"Optimizer:\", optimizer)\n",
    "print(\"-----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, save_path):\n",
    "    best_val_loss = np.inf\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for i, (inputs, trunk, targets) in enumerate(train_loader):\n",
    "            inputs, trunk, targets = inputs.to(device), trunk.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, trunk)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            break\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        for i, (inputs, trunk, targets) in enumerate(val_loader):\n",
    "            inputs, trunk, targets = inputs.to(device), trunk.to(device), targets.to(device)\n",
    "            outputs = model(inputs, trunk)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}, Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined output shape: torch.Size([16, 65341, 128])\n",
      "Final output shape: torch.Size([16, 65341, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, save_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs, trunk)\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch-env/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch-env/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, scaler, device='cuda'):\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for branch_batch, trunk_batch, target_batch in test_loader:\n",
    "            branch_batch, trunk_batch, target_batch = (\n",
    "                branch_batch.to(device),\n",
    "                trunk_batch.to(device),\n",
    "                target_batch.to(device),\n",
    "            )\n",
    "            output = model(branch_batch, trunk_batch)\n",
    "            all_preds.append(output.cpu().numpy())\n",
    "            all_targets.append(target_batch.cpu().numpy())\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "    \n",
    "    \n",
    "    print(\"All predictions shape before reshape:\", all_preds.shape)\n",
    "    print(\"All targets shape before reshape:\", all_targets.shape)\n",
    "\n",
    "    # Reshape to 2D (n_samples, n_features) for inverse scaling\n",
    "    all_preds = all_preds.reshape(all_preds.shape[0], -1)\n",
    "    all_targets = all_targets.reshape(all_targets.shape[0], -1)\n",
    "\n",
    "    print(\"All predictions shape after reshape:\", all_preds.shape)\n",
    "    print(\"All targets shape after reshape:\", all_targets.shape)\n",
    "\n",
    "    # Inverse scaling\n",
    "    all_preds = scaler.inverse_transform(all_preds)\n",
    "    all_targets = scaler.inverse_transform(all_targets)\n",
    "    \n",
    "    # Compute metrics for each sample\n",
    "    rmse, mae, r2, l2_error = [], [], [], []\n",
    "    for i in range(all_preds.shape[0]):\n",
    "        rmse.append(np.sqrt(np.mean((all_preds[i] - all_targets[i]) ** 2)))\n",
    "        mae.append(np.mean(np.abs(all_preds[i] - all_targets[i])))\n",
    "        r2.append(1 - np.sum((all_preds[i] - all_targets[i]) ** 2) / np.sum((all_targets[i] - np.mean(all_targets[i])) ** 2))\n",
    "        l2_error.append(np.linalg.norm(all_preds[i] - all_targets[i], 2))\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    rmse = np.array(rmse)\n",
    "    mae = np.array(mae)\n",
    "    r2 = np.array(r2)\n",
    "    l2_error = np.array(l2_error)\n",
    "    \n",
    "    # save the results to a file\n",
    "    results = np.stack((rmse, mae, r2, l2_error), axis=1)\n",
    "    save_path = os.path.join(save_dir, f'test_fcn_window_{window_size}_results.npy')\n",
    "    np.save(save_path, results)\n",
    "    print(f\"Results saved to {save_path}\")\n",
    "    \n",
    "    # Compute average metrics\n",
    "    rmse = np.mean(rmse)\n",
    "    mae = np.mean(mae)\n",
    "    r2 = np.mean(r2)\n",
    "    l2_error = np.mean(l2_error)\n",
    "\n",
    "    print(f\"Final Model Evaluation on Test Set:\")\n",
    "    print(f\"RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}, L2 Error: {l2_error:.4f}\")\n",
    "\n",
    "    return rmse, mae, r2, l2_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate the model\n",
      "-----------------------------------------\n",
      "All predictions shape before reshape: (336, 65341, 1)\n",
      "All targets shape before reshape: (336, 65341, 1)\n",
      "All predictions shape after reshape: (336, 65341)\n",
      "All targets shape after reshape: (336, 65341)\n",
      "Results saved to single_branch/test_fcn_window_30_results.npy\n",
      "Final Model Evaluation on Test Set:\n",
      "RMSE: 0.0000, MAE: 0.0000, R²: 0.9997, L2 Error: 0.0122\n",
      "(4.7732476e-05, 3.9387345e-05, 0.9996720497903901, 0.012201323)\n"
     ]
    }
   ],
   "source": [
    "# load the best model\n",
    "model = init_model().to(device)\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "# evaluate the model\n",
    "print(\"Evaluate the model\\n-----------------------------------------\")\n",
    "print(evaluate_model(model, test_loader, scaler_target, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch-env]",
   "language": "python",
   "name": "conda-env-.conda-pytorch-env-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
