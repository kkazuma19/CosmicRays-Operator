{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908bd9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc0e3b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# compactly add project src and analysis/zero-shot to sys.path if not already present\n",
    "for rel in ('src', 'analysis/finetuning', 'analysis/forecasting'):\n",
    "    p = os.path.abspath(os.path.join(os.getcwd(), rel))\n",
    "    if p not in sys.path:\n",
    "        sys.path.append(p)\n",
    "\n",
    "# now imports that rely on those paths\n",
    "from utils import SequentialDeepONetDataset\n",
    "from helper import load_model_experiment, convert2dim, fit, init_model\n",
    "from forecasting_analysis import create_windows_forecasting_with_index\n",
    "from finetune import create_contiguous_adaptation_set, create_eval_set_after_contiguous_adaptation, freeze_for_new_station_adaptation, expand_lstm_input_dim_correct, mask_new_station, fine_tune_adapt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "989e192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original input sensor data: neutron monitor data \n",
    "input_sensor = np.load('data/neutron_data_22yrs.npy')\n",
    "\n",
    "# location\n",
    "trunk = np.load('data/grid_points_025.npy')\n",
    "\n",
    "# Normalize trunk input\n",
    "trunk[:, 0] = (trunk[:, 0] - np.min(trunk[:, 0])) / (np.max(trunk[:, 0]) - np.min(trunk[:, 0]))\n",
    "trunk[:, 1] = (trunk[:, 1] - np.min(trunk[:, 1])) / (np.max(trunk[:, 1]) - np.min(trunk[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2e4d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# \n",
    "# base_dir = Path(\"data/DoseNumpy025/10m\")\n",
    "# \n",
    "# years = range(2001, 2024)  # 2001â€“2023 inclusive\n",
    "# \n",
    "# arrays = []\n",
    "# for year in years:\n",
    "#     print(f\"Loading data for year: {year}\")\n",
    "#     fname = base_dir / f\"dose_{year}_10m.npy\"\n",
    "#     arr = np.load(fname)          # shape e.g. (T_year, H, W) or (T_year, N_points)\n",
    "#     arrays.append(arr)\n",
    "# \n",
    "# # Concatenate along time axis (axis=0)\n",
    "# dose_all = np.concatenate(arrays, axis=0)\n",
    "# \n",
    "# print(\"Per-year shape:\", arrays[0].shape)\n",
    "# print(\"Combined shape:\", dose_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4429d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.load('data/DoseNumpy025/dose_2001_2023_10m.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cc74074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: torch.Size([7641, 30, 12]) torch.Size([7641, 1038961])\n",
      "Validation set: torch.Size([365, 30, 12]) torch.Size([365, 1038961])\n",
      "Test set: torch.Size([365, 30, 12]) torch.Size([365, 1038961])\n"
     ]
    }
   ],
   "source": [
    "dates = pd.date_range(\"2001-01-01\", \"2023-12-31\", freq=\"D\")\n",
    "\n",
    "W, H = 30, 0\n",
    "X_all, y_all, tgt_idx = create_windows_forecasting_with_index(input_sensor, output, W, H)\n",
    "tgt_dates = dates[tgt_idx]\n",
    "\n",
    "train_mask = (tgt_dates <= pd.Timestamp(\"2021-12-31\"))\n",
    "val_mask   = (tgt_dates >= pd.Timestamp(\"2022-01-01\")) & (tgt_dates <= pd.Timestamp(\"2022-12-31\"))\n",
    "test_mask  = (tgt_dates >= pd.Timestamp(\"2023-01-01\")) & (tgt_dates <= pd.Timestamp(\"2023-12-31\"))\n",
    "\n",
    "X_train, y_train = X_all[train_mask], y_all[train_mask]\n",
    "X_val,   y_val   = X_all[val_mask],   y_all[val_mask]\n",
    "X_test,  y_test  = X_all[test_mask],  y_all[test_mask]\n",
    "\n",
    "# check shapes\n",
    "print(\"Train set:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set:\", X_val.shape, y_val.shape)\n",
    "print(\"Test set:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# remove unused variables to free memory\n",
    "del output\n",
    "del X_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "befd6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_input = MinMaxScaler()\n",
    "X_train_scaled = scaler_input.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_val_scaled   = scaler_input.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
    "X_test_scaled  = scaler_input.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2801bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on ALL training pixels (flattened)\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "# Transform sets\n",
    "y_train_scaled = scaler_target.fit_transform(y_train)[..., np.newaxis]\n",
    "\n",
    "del y_train\n",
    "\n",
    "y_val_scaled   = scaler_target.transform(y_val)[..., np.newaxis]\n",
    "\n",
    "del y_val\n",
    "\n",
    "y_test_scaled  = scaler_target.transform(y_test)[..., np.newaxis]\n",
    "\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f214ed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "train_dataset = SequentialDeepONetDataset(X_train_scaled, trunk, y_train_scaled)\n",
    "val_dataset   = SequentialDeepONetDataset(X_val_scaled,   trunk, y_val_scaled)\n",
    "test_dataset  = SequentialDeepONetDataset(X_test_scaled,  trunk, y_test_scaled)\n",
    "\n",
    "# create dataloaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#del X_train_scaled, y_train_scaled\n",
    "#del X_val_scaled,   y_val_scaled\n",
    "#del X_test_scaled,  y_test_scaled\n",
    "#import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac8eb44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forecasting_analysis import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce738d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f0a4ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001] Train Loss: 0.011317 | Val Loss: 0.001892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 002] Train Loss: 0.002380 | Val Loss: 0.001351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 003] Train Loss: 0.002312 | Val Loss: 0.001722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 004] Train Loss: 0.002104 | Val Loss: 0.002554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 005] Train Loss: 0.001936 | Val Loss: 0.001178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 006] Train Loss: 0.001952 | Val Loss: 0.001124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_gamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manalysis/finetuning/test_dev.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/bcnx/kazumak2/CosmicRays-Operator/analysis/forecasting/forecasting_analysis.py:146\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, device, num_epochs, lr, weight_decay, scheduler_step, scheduler_gamma, early_stop_patience, save_path)\u001b[0m\n\u001b[1;32m    143\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()                                  \u001b[38;5;66;03m# Backprop\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 146\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m branch_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# Average training loss\u001b[39;00m\n\u001b[1;32m    149\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    num_epochs=300,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-3,\n",
    "    scheduler_step=20,\n",
    "    scheduler_gamma=0.7,\n",
    "    early_stop_patience=20,\n",
    "    save_path=f\"analysis/finetuning/test_dev.pt\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
