Using device: cuda
Train input shape: (4017, 12)
Validation input shape: (4018, 12)
Test input shape: (365, 12)
Training with window_size=90
Check the shapes of the generated sequences
-----------------------------------------
Train input shape: torch.Size([3928, 90, 12])
Train target shape: torch.Size([3928, 65341, 1])
Validation input shape: torch.Size([3929, 90, 12])
Validation target shape: torch.Size([3929, 65341, 1])
Test input shape: torch.Size([276, 90, 12])
Test target shape: torch.Size([276, 65341, 1])
-----------------------------------------
Create DataLoaders for training and validation sets
-----------------------------------------
Batch size: 16
SequentialDeepONet(
  (branch_net): GRU(
    (gru): GRU(12, 128, num_layers=4, batch_first=True)
    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (fc): Linear(in_features=128, out_features=128, bias=True)
  )
  (trunk_net): FCN(
    (network): Sequential(
      (0): Linear(in_features=2, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
    )
  )
)
Set the hyperparameters
-----------------------------------------
single_branch/gru_window_90_1.pth
Number of epochs: 1000
Learning rate: 0.001
Patience: 5
Loss function: MSELoss()
-----------------------------------------
Epoch 1/1000, Train Loss: 0.130436, Validation Loss: 0.000653
Epoch 2/1000, Train Loss: 0.000660, Validation Loss: 0.001489
Epoch 3/1000, Train Loss: 0.000430, Validation Loss: 0.000156
Epoch 4/1000, Train Loss: 0.000269, Validation Loss: 0.000062
Epoch 5/1000, Train Loss: 0.000215, Validation Loss: 0.000050
Epoch 6/1000, Train Loss: 0.000157, Validation Loss: 0.000044
Epoch 7/1000, Train Loss: 0.000104, Validation Loss: 0.000037
Epoch 8/1000, Train Loss: 0.000091, Validation Loss: 0.000039
Epoch 9/1000, Train Loss: 0.000082, Validation Loss: 0.000032
Epoch 10/1000, Train Loss: 0.000081, Validation Loss: 0.000069
Epoch 11/1000, Train Loss: 0.000082, Validation Loss: 0.000027
Epoch 12/1000, Train Loss: 0.000088, Validation Loss: 0.000106
Epoch 13/1000, Train Loss: 0.000097, Validation Loss: 0.000054
Epoch 14/1000, Train Loss: 0.000095, Validation Loss: 0.000556
Epoch 15/1000, Train Loss: 0.000098, Validation Loss: 0.000039
Epoch 16/1000, Train Loss: 0.000094, Validation Loss: 0.000015
Epoch 17/1000, Train Loss: 0.000078, Validation Loss: 0.000025
Epoch 18/1000, Train Loss: 0.000143, Validation Loss: 0.000019
Epoch 19/1000, Train Loss: 0.000072, Validation Loss: 0.000059
Epoch 20/1000, Train Loss: 0.000158, Validation Loss: 0.000013
Epoch 21/1000, Train Loss: 0.000081, Validation Loss: 0.000513
Epoch 22/1000, Train Loss: 0.000111, Validation Loss: 0.000053
Epoch 23/1000, Train Loss: 0.000123, Validation Loss: 0.000012
Epoch 24/1000, Train Loss: 0.000151, Validation Loss: 0.000114
Epoch 25/1000, Train Loss: 0.000104, Validation Loss: 0.000011
Epoch 26/1000, Train Loss: 0.000187, Validation Loss: 0.000072
Epoch 27/1000, Train Loss: 0.000074, Validation Loss: 0.000331
Epoch 28/1000, Train Loss: 0.000152, Validation Loss: 0.000041
Epoch 29/1000, Train Loss: 0.000079, Validation Loss: 0.000014
Epoch 30/1000, Train Loss: 0.000088, Validation Loss: 0.000025
Early stopping at epoch 30
Training completed!
All predictions shape after reshape: (276, 65341)
All targets shape after reshape: (276, 65341)
Predictions and targets saved to single_branch/array/gru_window_90_preds_targets_1.npy
Results saved to single_branch/array/gru_window_90_results_1.npy
Final Model Evaluation on Test Set:
RMSE: 0.0000, MAE: 0.0000, R²: 0.9997, L2 Error: 0.0111
(4.3396503e-05, 3.51336e-05, 0.9997496675350176, 0.0110929655)
noisy
All predictions shape after reshape: (276, 65341)
All targets shape after reshape: (276, 65341)
Predictions and targets saved to single_branch/array/noise_gru_window_90_preds_targets_1.npy
Results saved to single_branch/array/noise_gru_window_90_results_1.npy
Final Model Evaluation on Test Set:
RMSE: 0.0001, MAE: 0.0001, R²: 0.9991, L2 Error: 0.0192
(7.507152e-05, 6.509757e-05, 0.9990986596847085, 0.019189693)
